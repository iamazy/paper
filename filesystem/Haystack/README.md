# Finding a needle in Haystack: Facebook’s photo storage

**简述**: 本文介绍了 Haystack，一种针对 Facebook 的图片应用程序进行优化的对象存储系统。Facebook 目前存储超过 2600 亿张图片，相当于超过 20PB 的数据。用户每周上传 10 亿张新图片（约 60TB），在高峰期，Facebook 每秒提供超过 100 万张图片。Haystack 提供了比我们先前使用的基于 NFS 的网络附加存储设备更经济高效的解决方案。更关键的是，我们观察到这种传统设计会因元数据查找而产生过多的磁盘操作。我们通过精简每张图片的元数据，使 Haystack 存储机器能够在主内存中执行所有元数据查找。这种选择保留了用于读取实际数据的磁盘操作，从而增加了整体吞吐量。

-------

## 1. 介绍

分享图片是 Facebook 最受欢迎的功能之一。迄今为止，用户上传了超过 650 亿张图片，使 Facebook 成为全球最大的图片分享网站。对于每张上传的图片，Facebook 会生成并存储四个不同尺寸的图片，这相当于超过 2600 亿张图片和超过 20PB 的数据。每周用户上传 10 亿张新图片（约 60TB），在高峰期，Facebook 每秒提供超过 100 万张图片。我们预计未来这些数字将会增加，图片存储对于 Facebook 的基础设施构成了重大挑战。

本文介绍了 Haystack 的设计和实现，这是 Facebook 过去 24 个月中一直在使用的图片存储系统。Haystack 是一个为分享图片而设计的对象存储系统 [7, 10, 12, 13, 25, 26]，其中数据只写入一次，经常读取，从不修改，很少删除。我们为图片工作负载而设计了自己的存储系统，因为传统的文件系统在我们的工作负载下性能较差。

根据以往经验，传统的基于 Posix [21] 的文件系统的缺点在于目录和每个文件的元数据。对于照片应用程序，大多数此类元数据（例如权限）未被使用，因此会浪费存储容量。然而，更重要的成本是必须从磁盘读取文件的元数据到内存中，以便找到文件本身。虽然在小规模上不重要，但在数十亿张照片和 PB 级数据的情况下，访问元数据是吞吐量的瓶颈。我们发现，这是使用基于 NFS 的网络附加存储（NAS）设备时的主要问题。读取单个照片需要多次磁盘操作：一次（或通常多次）将文件名转换为 inode 编号，另一次从磁盘中读取 inode，最后一次读取文件本身。简而言之，使用磁盘 I/O 进行元数据操作是我们读取吞吐量的限制因素。实际上，这个问题还会带来额外的成本，因为我们必须依赖内容交付网络（CDN），例如 Akamai [2]，来服务大部分的读取流量。

鉴于传统方法的缺点，我们设计了 Haystack，以实现以下四个主要目标：

- **高吞吐量和低延迟**：我们的照片存储系统必须跟得上用户的请求。超出我们处理能力的请求要么被忽略，这对用户体验来说是不可接受的，要么由 CDN 处理，这既昂贵又达到了收益递减的点。此外，为了提供良好的用户体验，应该能够快速地提供照片。Haystack 通过每次读取最多只需要一次磁盘操作来实现高吞吐量和低延迟。我们通过将所有元数据存储在主内存中来实现这一点，并通过大幅减少每张照片在磁盘上查找所需的元数据来实现实用性。

- **容错性**：在大规模系统中，故障每天都会发生。我们的用户依赖于他们的照片可用，并且不应该遇到错误，尽管不可避免地会发生服务器崩溃和硬盘故障。可能会发生整个数据中心断电或跨国链接断开的情况。Haystack 将每张照片复制到地理位置不同的位置。如果我们丢失了一台机器，我们会引入另一台机器来替代它，并根据需要复制数据以进行冗余。

- **成本效益**：与我们之前基于 NFS 的方法相比，Haystack 性能更好，成本更低。我们通过两个维度量化我们的节省：Haystack 每可用存储 1 TB 的成本和每可用存储 1 TB 的读取速率。相比于 NAS 设备，在 Haystack 中，每个可用的 TB 成本降低约 28％，并且每秒处理的读取数提高约 4 倍。

- **简单易用**：在生产环境中，我们不能过分强调实现和维护简单的设计优点。由于 Haystack 是一个新系统，缺乏多年的生产级测试，因此我们特别注意保持其简单性。这种简单性使我们能够在几个月而不是几年内构建和部署一个可用的系统。

本文描述了我们从构思到实现一种每天服务数十亿张图像的生产级系统的经验。我们的三个主要贡献是：

- Haystack：一种针对高效存储和检索数十亿张照片进行优化的对象存储系统。
- 在构建和扩展一种廉价、可靠且可用的照片存储系统方面所学到的教训。
- 对 Facebook 照片分享应用程序的请求进行了表征。

我们将本文其余部分组织如下。第 2 节提供了背景，并强调了我们之前架构中面临的挑战。我们在第 3 节中描述了 Haystack 的设计和实现。第 4 节表征了我们的照片读写工作负载，并证明了 Haystack 满足我们的设计目标。我们在第 5 节中与相关工作进行比较，并在第 6 节中总结本文。

## 2. 背景 & 之前的设计

在本节中，我们描述了 Haystack 出现之前存在的架构，并强调了我们所学到的主要经验教训。由于篇幅限制，我们在讨论之前的设计时省略了生产级部署的一些细节。

### 2.1 背景

我们首先简要概述了网页服务器、内容传递网络（CDN）和存储系统相互交互以在流行网站上提供照片的典型设计。图 1 描绘了从用户访问包含图像的页面到她从磁盘上下载该图像的步骤。当用户访问页面时，浏览器首先向网页服务器发送一个 HTTP 请求，该服务器负责生成用于浏览器呈现的标记。对于每个图像，网页服务器构造一个 URL，指导浏览器从中下载数据的位置。对于流行的网站，该 URL 通常指向 CDN。如果 CDN 已经缓存了该图像，则 CDN 立即响应数据。否则，CDN 会检查 URL，其中包含足够的信息以从站点的存储系统检索照片。然后，CDN 更新其缓存数据并将图像发送到用户的浏览器。

![图1](./img/figure1.png)

### 2.2 基于 NFS 的设计

在我们的第一个设计中，我们使用基于 NFS 的方法实现了照片存储系统。虽然本小节的其余部分提供了更多关于该设计的细节，但我们所学到的主要教训是，单独的 CDN 并不能为社交网络站点上的照片提供实际的解决方案。CDN 确实能够有效地提供最热门的照片 - 头像和最近上传的照片，但像 Facebook 这样的社交网络站点也会为较不受欢迎（通常是较旧的）内容产生大量请求，我们称之为长尾。长尾请求占据了我们流量的相当一部分，几乎所有这些请求都会访问后端的照片存储主机，因为这些请求通常在 CDN 中未被命中。尽管缓存所有这些长尾照片非常方便，但由于需要非常大的缓存，这样做并不具有成本效益。

我们基于 NFS 的设计将每个照片存储为商用 NAS 设备上的单独文件。然后，一组名为 “Photo Store” 服务器的机器通过 NFS 挂载这些 NAS 设备导出的所有卷。图 2 说明了这种架构，并显示了 Photo Store 服务器处理图像的 HTTP 请求。从图像的 URL 中，Photo Store 服务器提取卷和文件的完整路径，通过 NFS 读取数据，并将结果返回给 CDN。

![图2](./img/figure2.png)

我们最初在每个 NFS 卷的每个目录中存储数千个文件，这导致即使读取单个图像也会产生过多的磁盘操作。由于 NAS 设备管理目录元数据的方式，将数千个文件放在一个目录中非常低效，因为目录的块映射表太大，无法被设备有效地缓存。因此，检索单个图像通常需要超过 10 次磁盘操作。将目录大小减小到每个目录数百张图像后，结果系统通常仍需要 3 次磁盘操作来获取图像：一次是将目录元数据读入内存，第二次是将 inode 加载到内存中，第三次是读取文件内容。

为了进一步减少磁盘操作，我们让 Photo Store 服务器明确地缓存 NAS 设备返回的文件句柄。当第一次读取文件时，Photo Store 服务器正常地打开文件，同时在 memcache 中缓存文件名到文件句柄的映射[18]。当请求一个已缓存文件句柄的文件时，Photo Store 服务器使用我们添加到内核中的自定义系统调用`open by filehandle`直接打开文件。遗憾的是，这个文件句柄缓存只提供了微小的改进，因为不受欢迎的照片不太可能被缓存在内存中。一种可能的解决方案是将所有文件句柄存储在 memcache 中，但这只解决了部分问题，因为它依赖于 NAS 设备在主内存中拥有所有的 inode，这对于传统的文件系统来说是一项昂贵的要求。我们从 NAS 方案中所学到的主要教训是，仅仅专注于缓存 - 无论是 NAS 设备的缓存还是像 memcache 这样的外部缓存 - 对于减少磁盘操作的影响非常有限。存储系统最终要处理较不受欢迎的照片的长尾请求，这些照片在 CDN 中不可用，因此很可能会在我们的缓存中未命中。

### 2.3 讨论

我们很难为何时（或何时不）构建自定义存储系统提供精确的指导。然而，我们认为让社区了解我们为什么决定构建 Haystack 仍然是有帮助的。

面对我们基于 NFS 的设计中的瓶颈，我们探索了是否构建类似于 GFS [9] 的系统。由于我们将大部分用户数据存储在 MySQL 数据库中，我们系统中文件的主要用例是工程师用于开发工作的目录、日志数据和照片。NAS 设备为开发工作和日志数据提供了非常好的价格/性能比。此外，我们利用 Hadoop [11] 处理极大的日志数据。但是 MySQL、NAS 设备和 Hadoop 都不太适合处理长尾照片。

我们面临的困境可以概括为现有存储系统缺乏适当的 RAM-to-disk 比率。然而，没有正确的比率。系统只需要足够的主内存，以便可以一次性缓存文件系统所有的元数据。在我们的基于 NAS 的方法中，一个照片对应一个文件，每个文件都需要至少一个 inode，而 inode 大小为几百个字节。在这种方法中，拥有足够的主内存是不划算的。为了实现更好的价格/性能比，我们决定构建一个自定义存储系统，以减少每个照片的文件系统元数据量，使得拥有足够的主内存比购买更多的 NAS 设备更具成本效益。

## 3 设计 & 实现

Facebook 使用 CDN 来提供热门图像，并利用 Haystack 有效地响应长尾照片请求。当网站在提供静态内容时存在 I/O 瓶颈，传统的解决方案是使用 CDN。CDN 承担足够的负荷，以便存储系统可以处理剩余的长尾请求。在 Facebook 中，CDN 需要缓存过多的静态内容，使得传统（且廉价）的存储方法不会受到 I/O 限制。

为了解决我们基于 NFS 的方法中的关键瓶颈 - `磁盘操作`，我们设计了 Haystack。我们知道，在不久的将来 CDN 无法完全解决我们的问题。我们处理较不受欢迎的照片请求可能需要磁盘操作，但希望将此类操作的数量限制为仅读取实际照片数据所需的操作。Haystack 通过大幅减少文件系统元数据所使用的内存来实现这一目标，从而使将所有这些元数据保存在主内存中变得实用。

回想一下，相比于合理缓存的文件系统元数据，将一个照片存储在一个文件中可能会产生更多的元数据。Haystack 采取了一种简单的方法：它将多个照片存储在一个文件中，因此维护非常大的文件。我们展示了这种简单的方法非常有效。此外，我们认为它的简单性是它的优势，有利于快速实现和部署。下面我们将讨论这种核心技术及其周围的架构组件如何提供可靠和可用的存储系统。在 Haystack 的以下描述中，我们区分两种元数据。应用程序元数据描述构造浏览器可以用于检索照片的 URL 所需的信息。文件系统元数据标识主机检索存储在该主机磁盘上的照片所需的数据。

### 3.1 概述

Haystack 的体系结构包括三个核心组件：`Haystack Store`、`Haystack Directory`和`Haystack Cache`。为了简洁起见，我们使用省略了 “Haystack” 的方式来指代这些组件。Store 封装了照片的持久存储系统，并且是管理照片文件系统元数据的唯一组件。我们通过物理卷来组织 Store 的容量。例如，我们可以将服务器的 10TB 容量组织成 100 个提供 100GB 存储的物理卷。我们还将不同机器上的物理卷分组到逻辑卷中。当 Haystack 将照片存储在逻辑卷上时，该照片会被写入所有相应的物理卷。这种冗余性可以有效减轻由于硬盘故障、磁盘控制器错误等原因导致的数据丢失。Directory 维护逻辑到物理映射以及其他应用程序元数据，例如每个照片所在的逻辑卷和具有可用空间的逻辑卷。Cache 作为我们的内部 CDN，为 Store 遮蔽了热门照片的请求，并且如果上游 CDN 节点失败且需要重新获取内容，Cache 提供了隔离机制。

![图3](./img/figure3.png)

图 3 说明了 Store、Directory 和 Cache 组件如何适应用户浏览器、Web 服务器、CDN 和存储系统之间的典型交互。在 Haystack 架构中，浏览器可以被指示访问 CDN 或 Cache。请注意，虽然 Cache 本质上是CDN，但为了避免混淆，我们使用 “CDN” 来指代外部系统，“Cache” 来指代我们内部缓存照片的系统。拥有一个内部缓存基础设施使我们能够减少对外部 CDN 的依赖。

当用户访问页面时，Web 服务器使用 Directory 来构建每个照片的 URL。该 URL 包含多个信息，每个信息对应于从用户浏览器请求 CDN（或 Cache）到最终从存储器中检索照片的步骤序列。一个指示浏览器访问 CDN 的典型 URL 如下所示：

> http://⟨CDN⟩/⟨Cache⟩/⟨Machine id⟩/⟨Logical volume, Photo⟩

URL 的第一部分指定从哪个 CDN 请求照片。CDN 可以仅使用 URL 的最后一部分（逻辑卷和照片ID）在内部查找照片。如果 CDN 无法定位照片，则它会从 URL 中删除 CDN 地址，并请求 Cache。Cache 进行类似的查找以找到照片，并在未命中时从 URL 中删除 Cache 地址，并从指定的 Store 机器请求照片。直接发送到 Cache 的照片请求具有类似的工作流程，但 URL 缺少特定的 CDN 信息。

![图4](./img/figure4.png)

图 4 说明了 Haystack 中的上传路径。当用户上传照片时，首先将数据发送到 Web 服务器。接下来，该服务器从目录中请求一个可写的逻辑卷。最后，Web 服务器为照片分配一个唯一的 ID，并将其上传到映射到分配的逻辑卷的每个物理卷中。

### 3.2 Haystack Directory

Directory 具有四个主要功能。首先，它提供了逻辑卷到物理卷的映射。Web 服务器在上传照片时使用此映射，也在构建页面请求的图像 URL 时使用此映射。其次，Directory 在逻辑卷上进行写负载平衡，物理卷上进行读负载平衡。第三，Directory 确定照片请求应由 CDN 还是 Cache 处理。此功能使我们能够调整对 CDN 的依赖程度。第四，Directory 确定那些逻辑卷是只读的，这可能是由于操作上的原因或由于这些卷已达到其存储容量。为了操作方便，我们以机器为粒度，将卷标记为只读状态。

当我们通过添加新机器来增加 Store 的容量时，这些机器是可写的，只有可写的机器才能接收上传请求。随着时间的推移，这些机器上的可用容量会减少。当一台机器的容量用尽时，我们会将其标记为只读。在下一小节中，我们将讨论这种区别对 Cache 和 Store 的微妙影响。

Directory 是一个相对简单的组件，它将其信息存储在一个有副本的数据库中，通过 PHP 接口访问该数据库，利用 memcache 来降低延迟。如果我们在 Store 上失去了数据，我们会删除相应的映射条目，并在新的 Store 上线时进行替换。

### 3.3 Haystack Cache

Cache 从 CDN 和用户浏览器接收照片的 HTTP 请求。我们将 Cache 组织为分布式哈希表，并使用照片的 ID 作为定位缓存数据的键。如果 Cache 无法立即响应请求，则 Cache 从 URL 中识别的 Store 机器获取照片，并根据需要回复 CDN 或用户的浏览器。

现在，我们强调 Cache 的一个重要方面。仅当满足以下两个条件时，Cache 才会缓存照片：（a）请求直接来自用户而不是 CDN，（b）照片从可写的 Store 机器中提取。第一个条件的理由是，我们使用基于 NFS 的设计经验表明，post-CDN 缓存是无效的，因为在 CDN 中未命中的请求不太可能在我们的内部缓存中命中。第二个条件的推理是间接的。我们使用 Cache 来保护可写的 Store 机器免受读取，因为有两个有趣的属性：照片在上传后不久会被大量访问，并且我们的文件系统通常在执行读取或写入时表现更好，而不是同时进行读写（请参见第 4.1 节）。因此，如果没有 Cache，可写的 Store 机器将看到最多的读取。鉴于这一特点，我们计划实现的一种优化是在我们预计这些照片很快经常被读取时，主动将最近上传的照片推入 Cache 中。

### 3.4 Haystack Store

Store 机器接口意图保持简单。读取操作会提出非常具体的请求，该接口请求以指定的照片 ID、逻辑卷 ID 和特定的 Store 机器为参数，用于获取对应的照片。如果找到该照片，机器将返回该照片。否则，机器将返回一个错误。

每个 Store 机器管理多个物理卷。每个卷可容纳数百万张照片。为了方便起见，读者可以将物理卷视为一个非常大的文件（100 GB），保存为 “/hay/haystack_\<logical volume id\>”。Store 机器可仅使用相应逻辑卷的 ID 和照片所在的文件偏移量快速访问照片。这是 Haystack 设计的关键：在不需要磁盘操作的情况下检索特定照片的文件名、偏移量和大小。Store 机器为其管理的每个物理卷保持打开的文件描述符，并且还保留一个内存映射，将照片 ID 映射到文件系统元数据（即文件名、偏移量和字节数），这些元数据对于检索该照片至关重要。

![图5](./img/figure5.png)